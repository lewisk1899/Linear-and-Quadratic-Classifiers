# Lewis Koplon
# ECE 523 Engineering Applications of Machine Learning and Data Analytics
# Professor Ditzler
# Linear and Quadratic Classifiers
import math

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D


def normal_samples():
    samples = 100
    # distribution 1
    mean_1 = np.array([0, 0])  # mean vector which is the 0 vector for simplicity
    mean_2 = np.array([-2.6, -2])
    mean_3 = np.array([2, 3])
    covariance_matrix = np.identity(2)  # covariance matrix is the identity matrix
    # we will be sampling from a standard normal distribution, this will produce 100 sample with d features [x1, x2,
    # x3... xd]
    dataset1 = np.array(np.random.multivariate_normal(mean_1, covariance_matrix, samples))  # produce a 30 by 2 matrix
    dataset2 = np.array(np.random.multivariate_normal(mean_2, covariance_matrix, samples))  # produce a 30 by 2 matrix
    dataset3 = np.array(np.random.multivariate_normal(mean_3, covariance_matrix, samples))  # produce a 30 by 2 matrix

    return dataset1, dataset2, dataset3  # returning the data generated by each distribution to another function for
    # mean estimation


def learn_parameters():
    x1, x2, x3 = normal_samples()  # produce the training data set, so we can estimate our mean and covariance
    # here is the process of estimating the mean and covariance
    # we need to learn the parameters of the three distributions to then use in the three discriminants
    mu_1 = x1.T.mean(axis=1)  # finds the distribution for first distribution
    print("Mean vector for distribution 1:", mu_1)
    cov_1 = np.cov(x1.T)
    print("Covariance Matrix for distribution 1:", cov_1)
    mu_2 = x2.T.mean(axis=1)  # finds the distribution for second distribution
    print("Mean vector for distribution 2:", mu_2)
    cov_2 = np.cov(x2.T)
    print("Covariance Matrix for distribution 1:", cov_2)
    mu_3 = x3.T.mean(axis=1)  # finds the distribution for third distribution
    print("Mean vector for distribution 3:", mu_3)
    cov_3 = np.cov(x3.T)  # estimate covariance
    print("Covariance Matrix for distribution 1:", cov_3)
    return mu_1, cov_1, mu_2, cov_2, mu_3, cov_3


def test_samples():
    samples = 100
    # distribution 1
    mean_1 = np.array([0, 0])  # mean vector which is the 0 vector for simplicity
    mean_2 = np.array([-2.6, -2])
    mean_3 = np.array([2, 3])
    covariance_matrix = np.identity(2)  # covariance matrix is the identity matrix
    dataset1 = np.array(np.random.multivariate_normal(mean_1, covariance_matrix, samples))  # produce a 30 by 2 matrix
    dataset2 = np.array(np.random.multivariate_normal(mean_2, covariance_matrix, samples))  # produce a 30 by 2 matrix
    dataset3 = np.array(np.random.multivariate_normal(mean_3, covariance_matrix, samples))  # produce a 30 by 2 matrix
    # creating one large dataset
    dataset = np.concatenate((dataset1, dataset2, dataset3), axis=0)
    return dataset


def classifier(sample, mean_i, cov_i, prob_of_class_i):
    # classifier that returns a scalar value
    g_x_i = -.5 * np.matmul(np.matmul((sample - mean_i), np.linalg.inv(cov_i)), (sample - mean_i).T) \
            - .5 * 2 * np.log(2 * np.pi) - .5 * np.log(np.linalg.det(cov_i)) + np.log(prob_of_class_i)
    return g_x_i


def test():
    prob_of_class_1 = prob_of_class_2 = prob_of_class_3 = .33  # equal amount of data points per class
    mean_1, cov_1, mean_2, cov_2, mean_3, cov_3 = learn_parameters()  # learn the parameters
    # make easier for transposing
    mean_1 = mean_1[np.newaxis]
    mean_2 = mean_2[np.newaxis]
    mean_3 = mean_3[np.newaxis]
    classified_data = []
    test_data = test_samples()
    # pass each data point through the discriminant and compare the scalars, the highest will mean the that data point belongs to that class
    for sample in test_data:
        sample = sample[np.newaxis]
        g_x_1 = classifier(sample, mean_1, cov_1, prob_of_class_1)
        g_x_2 = classifier(sample, mean_2, cov_2, prob_of_class_2)
        g_x_3 = classifier(sample, mean_3, cov_3, prob_of_class_3)
        classified_data.append((sample, g_x_1, g_x_2, g_x_3))  # data preprocessing and such
    # what class does this belong to?
    class_1 = class_2 = class_3 = []
    # assigning to classes
    for sample in classified_data:
        print("Class 1:")
        print(sample[0][0])
        print(sample[1][0])
        print("##########################################")
        print("Class 2:")
        print(sample[0][0])
        print(sample[2][0])
        print("##########################################")
        print("Class 3:")
        print(sample[0][0])
        print(sample[3][0])
        print("##########################################")
        # assign the sample to the class it belongs with
        if sample[1][0] >= sample[2][0] and sample[1][0] >= sample[3][0]:
            class_1.append(sample[0])
        elif sample[2][0] >= sample[1][0] and sample[2][0] >= sample[3][0]:
            class_2.append(sample[0])
        elif sample[3][0] >= sample[1][0] and sample[3][0] >= sample[2][0]:
            class_3.append(sample[0])
    graph(class_1, class_2, class_3)  # using matplotlib to help visualize said data


# working
def graph(class_1, class_2, class_3):
    fig, ax = plt.subplots()
    print(" Data Points this algorithm thinks is generated by class_1", len(class_1))
    print(" Data Points this algorithm thinks is generated by class_2", len(class_2))
    print(" Data Points this algorithm thinks is generated by class_3", len(class_3))
    for point in class_1:
        print("x1 = ", point[0][0], "x2 =", point[0][1])
        scatter = ax.scatter(point[0][0], point[0][1], c='#1f77b4')  # Blue
    for point in class_2:
        print("x1 = ", point[0][0], "x2 =", point[0][1])
        ax.scatter(point[0][0], point[0][1], c='#ff7f0e')
    for point in class_3:
        print("x1 = ", point[0][0], "x2 =", point[0][1])
        ax.scatter(point[0][0], point[0][1], c='#2ca02c')  # Green
    plt.xlabel("X1")
    plt.ylabel("X2")
    ax.set(xlim=(-6, 6), xticks=np.arange(-6, 6), ylim=(-6, 6), yticks=np.arange(-6, 6))
    legend_elements = [Line2D([0], [0], linewidth=0, marker='.', mfc='#1f77b4', mec='#1f77b4', label='Class 1',
                              markersize=10),
                       Line2D([0], [0], linewidth=0, marker='.', mfc='#ff7f0e', mec='#ff7f0e', label='Class 2',
                              markersize=10),
                       Line2D([0], [0], linewidth=0, marker='.', mfc='#2ca02c', mec='#2ca02c', label='Class 3',
                              markersize=10)]
    ax.legend(handles=legend_elements, loc='upper right')
    ax.grid(True)
    plt.show()


def mahalanobis_distance(point, mu, cov):
    return np.matmul(np.matmul((point - mu), np.linalg.inv(cov)), (point - mu).T)[0]


def naive_bayes(x, mean, cov):
    # P(w_j) = (total of number of points from this class)/(total number of points)
    p_w_j = .3333333
    # P(w)*P(x|w) = P(w_j) * (product from 1 to p) of (p(x_i|w))
    naive_bayes_result = p_w_j * (p_x_i_given_w(x[0], mean, cov) + p_x_i_given_w(x[1], mean, cov))
    # (p(x_i|w)) is a gaussian probability distribution


def p_x_i_given_w(x_i, mean, cov):
    return (1 / (math.sqrt(2 * np.pi * cov ** 2))) * np.exp ** (-1 * ((x_i - mean) ** 2) / (2 * cov ** 2))


def main():
    test()


main()
